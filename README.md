### Predicting Diffusion Reach Probabilities via Representation Learning on Social Networks

The code, replication steps, and a sample replication on the *Email-Eu-core* network is available for the following paper.
___
_If you find this useful for your research, please consider citing:_

GÃ¼rsoy, F., and A. O. Durahim. "Predicting Diffusion Reach Probabilities via Representation Learning on Social Networks." Fifth International Management Information Systems Conference. 2018.

Full Text: https://arxiv.org/abs/1901.03629

    @inproceedings{gursoy2018predicting,
      title={Predicting Diffusion Reach Probabilities via Representation Learning on Social Networks},
      author={G{\"u}rsoy, F and Durahim, AO},
      booktitle={Fifth International Management Information Systems Conference},
      year={2018},
      doi={10.6084/m9.figshare.7565894}
    }
___ 

*Feel free to email me with regard to the paper or the code.*

**CascadeGenerator.py** contains the code for generating cascade based on Independent Cascade Model (ICM).

**Prediction.py** contains the code for calculating scores for the benchmark and prediction models according to the proposed methodology.

**email_edgeweight.txt** contains the *Email-Eu-core* network dataset that we used in our paper. The weights are randomly assigned between 0 and 0.1.

**cascades_email** contains the cascades generated for the *email_edgeweight.txt* by *CascadeGenerator.py*.

**embeddings_email.csv** contains the embeddings generated by *node2vec*. Number of features (i.e., dimensionality) is 128.

**results.txt** contains the accuracy scores for 2 benchmarks and 2 machine learning models.

Details can be seen in the following replication steps, or by studying the code.


### Replication Steps

**In *CascadeGenerator.py*, specify the following input parameters at lines 113-115, and run the whole file.** 

*r* : number of cascades to be started from each node

*edgelist* : the input graph file with edge weights

*cascadefilename* : #name of the file for storing output cascades


**Generate node embeddings.** 

You can use any network representation learning (network embedding) method as you like. In the paper, we have used node2vec. Code and instructions for *node2vec* is available at https://github.com/aditya-grover/node2vec. 

Do not give link weights as input to the method since we assume that the weights are unknown to us. You can remove the weight column from the graph file and then use it as an input for *node2vec*.

At the end of this step, you should obtain a file with an embedding for each node. If the graph contains *n* nodes, then your file here should have *n* rows. 



**In *Prediction.py*, specify the following input parameters at lines 17-23, and run the whole file.**

*n* : number of nodes in the graph

*r* : number of cascades starting from each node

*dimension* : number of dimensions/features for the node embeddings

*trainingsizemultiplier* : the portion of cascades that is available (that we are going to learn our labels from)

*cascadefilename* : 'cascades_email' #name of the cascades output file generated by *CascadeGenerator.py*

*embfile* : the file name for embeddings generated by node2vec or other methods

*outputfile* : name of the file for storing accuracy scores


**Observe the output file.**

The first line is the score for the very naive benchmark which we have not reported in our paper.

The second line is the score for the benchmark which we have reported in our paper.

The third and fourth lines are the scores for GradientBoostingRegressor and MLPRegressor, respectively.

Two additional files are generated which show predicted and actual probabilty values for comparison.


**Further directions.**

Use different machine learning models (e.g., ensemble learning methods) and tune their parameters. You can notice that in our experiments, we have not event tuned the parameters of the models we employed but rather used them in their default settings. If a better model with tuned parameters is utilized, the end results are likely to be better than what we presented in our paper.

Use different network representation learning (network embedding) methods and tune their parameters. You can notice that in our experiments, we only used *node2vec* and only tried one parameter setting. If a better representation learning method is selected and/or its parameters are tuned, the end results are likely to be  better than what we presented in our paper.


### Citing
